{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Reference:\n",
    "https://atcold.github.io/pytorch-Deep-Learning/"
   ],
   "metadata": {
    "id": "q1b5ShFwcKzR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch \r\n",
    "from torch import nn\r\n",
    "import torch.nn.functional as f\r\n",
    "import numpy as np "
   ],
   "outputs": [],
   "metadata": {
    "id": "EPYOi6rRcKzT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi head attention"
   ],
   "metadata": {
    "id": "sZHRVZt8cKzU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src='./self_attn_full.png' width=350 height=350>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class MultiHeadAttention(nn.Module):\r\n",
    "    def __init__(self, d_model, num_heads):\r\n",
    "        super().__init__()\r\n",
    "        \r\n",
    "        self.d_model = d_model # dimension of the input embedding\r\n",
    "        self.num_heads = num_heads # number of heads in multi-head attention\r\n",
    "        self.d_xq = self.d_xk = self.d_xv = self.d_model # dimension of query, key and value\r\n",
    "        \r\n",
    "        # Make sure the input embedding dimesion is divisible by num_heads\r\n",
    "        assert self.d_model % self.num_heads == 0\r\n",
    "        \r\n",
    "        # Dimension of heads\r\n",
    "        self.d_k = self.d_model // self.num_heads # d_model=512, num_heads=8, d_k=64        \r\n",
    "        \r\n",
    "        # Initial linear layers Q = Wq x X_q, K = W_k x X_k, V = W_v x X_v \r\n",
    "        self.W_q = nn.Linear(self.d_xq, self.d_model, bias=False)\r\n",
    "        self.W_k = nn.Linear(self.d_xk, self.d_model, bias=False)\r\n",
    "        self.W_v = nn.Linear(self.d_xv, self.d_model, bias=False)        \r\n",
    "        \r\n",
    "        # Final linear layer\r\n",
    "        self.W_h = nn.Linear(self.d_model, self.d_model, bias=False)\r\n",
    "        \r\n",
    "    def scaled_dot_product(self, Q, K, V):\r\n",
    "        ''' Scaled dot product to calculate self-attention '''\r\n",
    "        \r\n",
    "        print(\"Size of Q:\", Q.size())\r\n",
    "        print(\"Size of K:\", K.size())\r\n",
    "        print(\"Size of V:\", V.size())\r\n",
    "        # scaling - divide by square root of d_k\r\n",
    "        Q = Q / np.sqrt(self.d_k)  # (batch_size, num_heads, q_length, dim_per_head)\r\n",
    "        # dot-product\r\n",
    "        scores = torch.matmul(Q, K.transpose(2,3))  # (batch_size, num_heads, q_length, k_length)\r\n",
    "        \r\n",
    "        # Softmax of scores\r\n",
    "        A = nn.Softmax(dim=-1)(scores)  # (batch_size, num_heads, q_length, k_length)\r\n",
    "        print(\"Size of A:\", A.size())\r\n",
    "        \r\n",
    "        # Get the weighted average of values,V\r\n",
    "        H = torch.matmul(A, V)  # (batch_size, num_heads, q_length, dim_per_head)\r\n",
    "        print(\"Size of H:\", H.size())\r\n",
    "        \r\n",
    "        return H, A\r\n",
    "    \r\n",
    "    def split_heads(self, x):\r\n",
    "        ''' Split the embeddings into multiple heads '''\r\n",
    "        \r\n",
    "        batch_size = x.size(0)  # x -> (batch_size, seq_length, d_model)\r\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)   # (batch_size, num_heads, seq_length, dim_per_head)\r\n",
    "    \r\n",
    "    def group_heads(self, x): \r\n",
    "        ''' Combine the heads again to get batch_size X seq_length X d_model (num_heads x dim_per_head) '''\r\n",
    "        \r\n",
    "        batch_size = x.size(0)\r\n",
    "        return x.transpose(1,2).view(batch_size, -1, self.num_heads * self.d_k)     # (batch_size X seq_length X (num_heads x dim_per_head))\r\n",
    "    \r\n",
    "    def forward(self, X_q, X_k, X_v): # for self-attention X_q = X_k = X_v = X\r\n",
    "        \r\n",
    "        batch_size, seq_length, d_model = X_q.size()\r\n",
    "        \r\n",
    "        # Step 1: Linear layer and split heads\r\n",
    "        Q = self.split_heads(self.W_q(X_q))  # (batch_size, num_heads, q_length, dim_per_head)\r\n",
    "        K = self.split_heads(self.W_q(X_k))  # (batch_size, num_heads, k_length, dim_per_head)\r\n",
    "        V = self.split_heads(self.W_q(X_v))  # (batch_size, num_heads, v_length, dim_per_head)\r\n",
    "        \r\n",
    "        # Step 2: Scaled dot product \r\n",
    "        H_cat, A = self.scaled_dot_product(Q, K, V)  # (batch_size, num_heads, q_length, k_length) \r\n",
    "        \r\n",
    "        # Step 3: Combine the heads\r\n",
    "        H_cat = self.group_heads(H_cat)  # (batch_size, q_length, d_model)\r\n",
    "        H = self.W_h(H_cat)  # (batch_size, q_length, d_model)\r\n",
    "        \r\n",
    "        return H, A      "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Initiate the multi-head self-attention model\n",
    "mha = MultiHeadAttention(d_model=512, num_heads=8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def attention(Q, K, V):\n",
    "    ''' Print out attention scores and output '''\n",
    "    \n",
    "    temp_out, temp_attn = mha.scaled_dot_product(Q, K, V)\n",
    "    print('Attention weights are:', temp_attn.squeeze())\n",
    "    print('Output is:', temp_out.squeeze())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "test_K = torch.tensor(\n",
    "    [[10, 0, 0],\n",
    "     [ 0,10, 0],\n",
    "     [ 0, 0,10],\n",
    "     [ 0, 0,10]]\n",
    ").float()[None,None]\n",
    "\n",
    "test_V = torch.tensor(\n",
    "    [[   1,0,0],\n",
    "     [  10,0,0],\n",
    "     [ 100,5,0],\n",
    "     [1000,6,0]]\n",
    ").float()[None,None]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0, 10, 0]]\n",
    ").float()[None,None]\n",
    "attention(test_Q, test_K, test_V)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of Q: torch.Size([1, 1, 1, 3])\n",
      "Size of K: torch.Size([1, 1, 4, 3])\n",
      "Size of V: torch.Size([1, 1, 4, 3])\n",
      "Size of A: torch.Size([1, 1, 1, 4])\n",
      "Size of H: torch.Size([1, 1, 1, 3])\n",
      "Attention weights are: tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
      "Output is: tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The query focuses on second key and returned the second value. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "test_Q = torch.tensor([[0, 0, 10]])[None, None].float()  \n",
    "attention(test_Q, test_K, test_V)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of Q: torch.Size([1, 1, 1, 3])\n",
      "Size of K: torch.Size([1, 1, 4, 3])\n",
      "Size of V: torch.Size([1, 1, 4, 3])\n",
      "Size of A: torch.Size([1, 1, 1, 4])\n",
      "Size of H: torch.Size([1, 1, 1, 3])\n",
      "Attention weights are: tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
      "Output is: tensor([549.9979,   5.5000,   0.0000])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### If we give a query that matches two keys exactly, it focuses on the two keys equally and returns the average of the two values for those two keys. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "name": "15-transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}